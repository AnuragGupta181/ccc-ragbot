# -*- coding: utf-8 -*-
"""ccc-chatbot-custom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mwLX_EzAwOE8I8-tQcA3SDR3WYLCJTz_
"""


import os
from dotenv import load_dotenv
load_dotenv()


from uuid import uuid4
import json
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

FILE_Path = "src/public/data.json"
with open(FILE_Path, "r", encoding="utf-8") as f:
    parsed_data = json.load(f)

assert isinstance(parsed_data, dict), "parsed_data MUST be a dict"


SECTION_CONFIG = {
    "faqs": {"content_fields": ["question", "answer"]},
    "society_info": {"content_fields": ["name","established_year","about_us","who_we_are","coordinators"]},
    "domains": {"content_fields": ["domain", "details"]},
    "faculty": {"content_fields": ["name","department","email","experience","subjects","skills","description"]},
    "events": {"content_fields": ["name","date","date_iso","content","link"]},
    "alumni": {"content_fields": ["full_name","domain","social","graduation_year","status","summary"]},
    "fourth_year_members": {"content_fields": ["full_name","domain","social","graduation_year","status","summary"]},
    "third_year_members": {"content_fields": ["full_name","domain","social","graduation_year","status","summary"]},
    "second_year_members": {"content_fields": ["full_name","domain","social","status","summary"]},
}


def build_documents_from_section(section_name, section_data, content_fields):
    documents = []

    if isinstance(section_data, dict):
        section_data = [section_data]

    for item in section_data:
        if not isinstance(item, dict):
            continue

        # Convert list of tags to a comma-separated string
        tags = ", ".join(item.get("tags", []))

        content_parts = [
            str(item.get(field, "")).strip()
            for field in content_fields
            if item.get(field)
        ]

        page_content = " | ".join(content_parts).strip()
        if not page_content:
            continue

        documents.append(
            Document(
                page_content=page_content,
                metadata={
                    "source": section_name,
                    "tags": tags, # Tags are now a string
                },
            )
        )

    return documents


# -----------------------------
# Build + Split + Assign IDs
# -----------------------------
splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", ", ", " "],
)

all_docs=[]
all_split_documents = []
all_ids = []

for section_name, config in SECTION_CONFIG.items():
    section_data = parsed_data.get(section_name)
    if not section_data:
        continue

    base_docs = build_documents_from_section(
        section_name=section_name,
        section_data=section_data,
        content_fields=config["content_fields"],
    )
    all_docs.append(base_docs)

    split_docs = splitter.split_documents(base_docs)

    for i, doc in enumerate(split_docs):
        doc.metadata["chunk_index"] = i
        all_split_documents.append(doc)
        all_ids.append(f"{section_name}_{i}_{uuid4()}")

# print(f" total docs: {len(all_docs)}")
# print(f"âœ… Total chunks created: {len(all_split_documents)}")
# print(f"âœ… Total IDs created: {len(all_ids)}")

# Add Vector store
# vectorstore = InMemoryVectorStore(embedding=HuggingFaceEmbeddings())
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

embedding = HuggingFaceEmbeddings()
persist_dir = "./chroma_langchain_db"

vectorstore = Chroma(
    collection_name="ccc_data_collection",
    embedding_function=embedding,
    persist_directory=persist_dir,
)

# Only ingest if collection is empty
if vectorstore._collection.count() == 0:
    print("ðŸ“¦ Ingesting documents into vector store...")
    vectorstore.add_documents(
        documents=all_split_documents,
        ids=all_ids,
    )
else:
    print("âœ… Vector store already populated. Skipping ingestion.")

# results = vector_store.similarity_search(
#     "tukur",
#     k=5, # Increase k to retrieve more potential documents before filtering
#     filter={
#         "source": {"$in": ["alumni","fourth_year_members","third_year_members","second_year_members"]}
#     }
# )

# for r in results:
#     print(r.metadata["source"], r.metadata.get("chunk_index", "N/A"), "Tags:", r.metadata.get("tags", ""))
#     print(r.page_content)
#     print("-" * 50)

#retriver
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5},filter={
        "source": {"$in": ["alumni","fourth_year_members","third_year_members","second_year_members","faculty"]}
})

# print(retriever.invoke("Anurag"))

#retriver
members_retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5},filter={
        "source": {"$in": ["alumni","fourth_year_members","third_year_members","second_year_members","faculty"]}
})
retrive_faculty = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 2},filter={
        "source": {"$in": ["faculty"]}
})
retrive_alumni = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["alumni"]}
})
retrive_fourth_year_members = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["fourth_year_members"]}
})
retrive_third_year_members = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["third_year_members"]}
})
retrive_second_year_members = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["second_year_members"]}
})
retrive_info = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 1},filter={
        "source": {"$in": ["society_info","faqs"]}
})
retrive_domains = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["domains","society_info"]}
})
retrive_events = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["events","society_info","faqs"]}
})
retrive_faqs = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3},filter={
        "source": {"$in": ["faqs","society_info"]}
})

# from langchain.tools import tool

# @tool
# def retrieve_members(query: str) -> str:
#     """
#     Retrieve information about all Cloud Computing Cell (CCC) members.
#     Covers faculty, alumni, and student members across years.
#     """
#     docs = members_retriever.invoke(query)

#     if not docs:
#         return "No matching members found."

#     return "\n\n".join(
#         f"[{doc.metadata.get('source')}] {doc.page_content}"
#         for doc in docs
#     )

# retrieve_members.invoke({"query": "Anurag Gupta"})

from langchain_core.tools import create_retriever_tool

def make_retriever_tool(retriever, name: str, description: str):
    return create_retriever_tool(
        retriever=retriever,
        name=name,
        description=f"[TYPE:RAG] {description}"
    )

retrieve_members_tool = make_retriever_tool(
    members_retriever,
    name="retrieve_members",
    description=(
        "Retrieve information about all Cloud Computing Cell (CCC) members. "
        "This tool covers faculty, alumni, and current student members "
        "including fourth year, third year, and second year students. "
        "Use this when the query is broad or when the user does not specify "
        "a particular role or year. Suitable queries include full names "
        "(e.g., 'Full Name'), role-based searches like 'CCC members', "
        "'faculty and alumni', or general membership-related questions."
    )
)
retrieve_faculty_tool = make_retriever_tool(
    retrive_faculty,
    name="retrieve_faculty",
    description=(
        "Retrieve information specifically about CCC faculty members. "
        "Use names, departments, subjects, skills, or roles such as "
        "'faculty coordinator' or 'cloud computing faculty'."
    )
)

retrieve_alumni_tool = make_retriever_tool(
    retrive_alumni,
    name="retrieve_alumni",
    description=(
        "Retrieve details about CCC alumni. "
        "Useful for queries related to alumni names, graduation year, domains, "
        "current status, or social links."
    )
)
retrieve_fourth_year_members_tool = make_retriever_tool(
    retrive_fourth_year_members,
    name="retrieve_fourth_year_members",
    description=(
        "Retrieve information about CCC fourth year members. "
        "Use full names, domains, skills, or leadership-related queries."
    )
)

retrieve_third_year_members_tool = make_retriever_tool(
    retrive_third_year_members,
    name="retrieve_third_year_members",
    description=(
        "Retrieve information about CCC third year members. "
        "Use names, domains, interests, or participation-based queries."
    )
)
retrieve_second_year_members_tool = make_retriever_tool(
    retrive_second_year_members,
    name="retrieve_second_year_members",
    description=(
        "Retrieve information about CCC second year members. "
        "Best for beginner-level queries, new members, or early involvement details."
    )
)
retrieve_info_tool = make_retriever_tool(
    retrive_info,
    name="retrieve_society_info",
    description=(
        "Retrieve general information about the Cloud Computing Cell (CCC). "
        "Includes society overview, FAQs, purpose, coordinators, and foundational details."
    )
)
retrieve_domains_tool = make_retriever_tool(
    retrive_domains,
    name="retrieve_domains",
    description=(
        "Retrieve information about CCC domains such as cloud computing, DevOps, "
        "AI/ML, cybersecurity, or other technical focus areas."
    )
)
retrieve_events_tool = make_retriever_tool(
    retrive_events,
    name="retrieve_events",
    description=(
        "Retrieve information about CCC events including workshops, seminars, "
        "hackathons, and past or upcoming activities."
    )
)
retrieve_faqs_tool = make_retriever_tool(
    retrive_faqs,
    name="retrieve_faqs",
    description=(
        "Retrieve answers to frequently asked questions related to CCC. "
        "Use for queries about membership, registration, eligibility, "
        "events, or general doubts."
    )
)

rag_tools = [
    retrieve_members_tool,
    retrieve_faculty_tool,
    # retrieve_third_year_members_tool,
    # retrieve_second_year_members_tool,
    # retrieve_fourth_year_members_tool,
    # retrieve_alumni_tool,
    retrieve_info_tool,
    retrieve_domains_tool,
    retrieve_events_tool,
    retrieve_faqs_tool,
]

from langchain_community.tools import DuckDuckGoSearchRun
search = DuckDuckGoSearchRun()

from langchain.tools import tool

@tool
def web_search(query: str) -> str:
    """Use this tool to search the web for recent information."""
    try:
        result = search.invoke(query)
        return result if isinstance(result, str) else str(result)
    except Exception as e:
        return f"Web search failed: {e}"

from langchain_community.utilities import OpenWeatherMapAPIWrapper
@tool
def get_weather(location: str) -> str:
    """Get the current weather for a given location, and return temperature in Celsius, AQI, humidity, wind speed etc."""
    try:
        weather = OpenWeatherMapAPIWrapper()
        result = weather.run(location)
        return result if isinstance(result, str) else str(result)
    except Exception as e:
        return f"Weather service error: {e}"
    

    
from langchain_tavily import TavilySearch

@tool
def tavily_search(query: str) -> str:
    """Search the web using Tavily for recent and reliable information."""
    tool = TavilySearch(
    max_results=5,
    topic="general",
    # include_answer=False,
    # include_raw_content=False,
    # include_images=False,
    # include_image_descriptions=False,
    # search_depth="basic",
    # time_range="day",
    # include_domains=None,
    # exclude_domains=None
    )
    return tool.invoke(query)


import os
from langchain_community.tools.riza.command import ExecPython
from langchain.tools import tool

@tool
def code_executor(code: str) -> str:
    """
    Execute Python code snippets in a secure sandbox and return the output.
    Use this tool for running short Python, JavaScript, PHP, Ruby code examples or calculations.
    """
    # Riza reads API key from environment
    if not os.getenv("RIZA_API_KEY"):
        return "Code execution service unavailable: Missing RIZA_API_KEY."

    executor = ExecPython()

    try:
        result = executor.invoke(code)

        # Normalize output
        if isinstance(result, dict):
            return result.get("stdout") or result.get("result") or str(result)

        return str(result)

    except Exception as e:
        return f"Code execution error: {str(e)}"

ACTION_TOOLS = [
    get_weather,
    web_search,
    tavily_search,
    code_executor,
]

tools = rag_tools + ACTION_TOOLS

from langchain_core.tools import create_retriever_tool

# retrieve_members.invoke({"query": "anurag"})

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

def get_latest_user_question(messages):
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            return msg.content
    raise ValueError("No HumanMessage found")

def get_latest_context_message(messages):
    for msg in reversed(messages):
        if isinstance(msg, ToolMessage):
            return msg
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            return msg
    raise ValueError("No context message found")

def get_latest_context(messages):
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            return msg.content
    raise ValueError("No context message found in state")


def get_latest_user_message(messages):
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            return msg
    raise ValueError("No HumanMessage found")


from langgraph.graph import MessagesState
from langchain_deepseek import ChatDeepSeek

response_model =  ChatDeepSeek(
    model="nvidia/nemotron-3-nano-30b-a3b:free",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    api_base="https://openrouter.ai/api/v1",
    extra_body={"reasoning": {"enabled": True}},
)



def generate_query_or_respond(state: MessagesState):
    """Call the model to generate a response based on the current state. Given
    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
    """
    messages = state["messages"]

    # âœ… Ensure last user message drives the decision
    if not isinstance(messages[-1], HumanMessage):
        # fallback: find most recent user message
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                messages = messages + [msg]
                break

    response = (
        response_model
        # highlight-next-line
        .bind_tools(tools)
        .invoke(messages)
    )

    return {"messages": [response]}


from pydantic import BaseModel, Field
from typing import Literal
from langchain_core.messages import ToolMessage

GRADE_PROMPT = (
    "You are a grader assessing relevance of a retrieved document to a user question. \n "
    "Here is the retrieved document: \n\n {context} \n\n"
    "Here is the user question: {question} \n"
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n"
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."
)


# highlight-next-line
class GradeDocuments(BaseModel):
    """Grade documents using a binary score for relevance check."""

    binary_score: str = Field(
        description="Relevance score: 'yes' if relevant, or 'no' if not relevant"
    )


grader_model = ChatDeepSeek(
    model="google/gemini-2.5-flash-lite-preview-09-2025",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    api_base="https://openrouter.ai/api/v1",
    extra_body={"reasoning": {"enabled": True}},
)



def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:

    messages = state["messages"]

    # âœ… MEMORY-SAFE extraction
    question = get_latest_user_question(messages)
    context = get_latest_context(messages)

    prompt = GRADE_PROMPT.format(
        question=question,
        context=context
    )

    response = (
        grader_model
        .with_structured_output(GradeDocuments)
        .invoke([{"role": "user", "content": prompt}])
    )

    score = response.binary_score.strip().lower()

    return "generate_answer" if score == "yes" else "rewrite_question"


REWRITE_PROMPT = (
    "Look at the input and try to reason about the underlying semantic intent / meaning.\n"
    "Here is the initial question:"
    "\n ------- \n"
    "{question}"
    "\n ------- \n"
    "Formulate an improved question:"
)


def rewrite_question(state: MessagesState):
    """Rewrite the original user question."""
    messages = state["messages"]

    last_user_msg = get_latest_user_message(messages)
    question = last_user_msg.content
    
    prompt = REWRITE_PROMPT.format(question=question)

    response = response_model.invoke(
        [HumanMessage(content=prompt)]
    )
    return {
        "messages": [
            HumanMessage(content=response.content)
        ]
    }

GENERATE_PROMPT = (
    "You are an assistant for question-answering tasks.\n\n"
    "Tool usage rules:\n"
    "- If the context comes from a RAG tool, answer strictly using that context.\n"
    "- If the context comes from a custom tool (web, weather, code execution, tavily), "
    "summarize the tool output clearly.\n"
    "- If the context is insufficient, say you don't know.\n\n"
    "Use a maximum of three concise sentences.\n\n"
    "Question:\n{question}\n\n"
    "Context:\n{context}"
)

CUSTOM_TOOL_NAMES = {
    "web_search",
    "get_weather",
    "tavily_search",
    "code_executor",
}

RAG_TOOL_NAMES = [
    "retrieve_members_tool",
    "retrieve_faculty_tool",
    # retrieve_third_year_members_tool,
    # retrieve_second_year_members_tool,
    # retrieve_fourth_year_members_tool,
    # retrieve_alumni_tool,
    "retrieve_info_tool",
    "retrieve_domains_tool",
    "retrieve_events_tool",
    "retrieve_faqs_tool",
]


def generate_answer(state: MessagesState):
    """Generate a final answer with tool-type awareness (memory-safe)."""

    messages = state["messages"]

    # âœ… Always latest user question
    question = get_latest_user_question(messages)

    # âœ… Context comes from tool or AI
    context_msg = get_latest_context_message(messages)
    context = context_msg.content

    # âœ… Detect tool type
    tool_type = "none"
    tool_name = None

    if isinstance(context_msg, ToolMessage):
        tool_name = context_msg.name
        if context_msg.name in RAG_TOOL_NAMES:
            tool_type = "rag"
        elif context_msg.name in CUSTOM_TOOL_NAMES:
            tool_type = "custom"

    prompt = GENERATE_PROMPT.format(
        question=question,
        context=context
    )

    response = response_model.invoke(
        [HumanMessage(content=prompt)]
    )

    # âœ… Attach metadata safely
    response.metadata = {
        "tool_type": tool_type,
        "tool_name": tool_name,
    }

    return {"messages": [response]}




from pydantic import BaseModel, Field
from typing import Literal
from langchain_core.messages import AIMessage

class SuggestPrompt(BaseModel):
    """List of short follow-up suggestions."""

    suggestion_list: list[str] = Field(
        description="2 to 3 follow-up suggestions, each 3â€“6 words long."
    )



grader_model = ChatDeepSeek(
    model="google/gemini-2.5-flash-lite-preview-09-2025",
    api_key=os.getenv("OPENROUTER_API_KEY"),
    api_base="https://openrouter.ai/api/v1",
    extra_body={"reasoning": {"enabled": True}},
)

SUGGESTION_PROMPT = """
You are an assistant that generates follow-up query suggestions.

Rules:
- Use the provided final answer.
- If the final answer contains a question, generate possible answers to that question.
- If the final answer is neutral, suggest queries related to Cloud Computing Cell (CCC) and Related to neutral answer.
- Generate between 2 and 3 suggestions.
- Each suggestion must contain 3 to 6 words.

Final Answer:
{final_answer}
"""


def next_suggestions(state):
    last_message = state["messages"][-1]

    if not isinstance(last_message, AIMessage):
        raise ValueError("Expected last message to be AIMessage")

    final_answer = last_message.content
    prompt = SUGGESTION_PROMPT.format(final_answer=final_answer)

    response = grader_model.with_structured_output(
        SuggestPrompt
    ).invoke(prompt)

    # âœ… Convert structured output â†’ text
    suggestions_text = "\n".join(
        f"- {s}" for s in response.suggestion_list
    )

    return {
        "messages": [
            AIMessage(
                content=f"Suggested follow-up questions:\n{suggestions_text}"
            )
        ]
    }


from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.prebuilt import tools_condition
from uuid import uuid4
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig

workflow = StateGraph(MessagesState)

# Define the nodes we will cycle between
workflow.add_node("generate_query_or_respond", generate_query_or_respond)
workflow.add_node("all_tools", ToolNode(tools))
workflow.add_node("rewrite_question", rewrite_question)
workflow.add_node("generate_answer", generate_answer)
# workflow.add_node("next_suggestions", next_suggestions)

workflow.add_edge(START, "generate_query_or_respond")

# Decide whether to retrieve
workflow.add_conditional_edges(
    "generate_query_or_respond",
    # Assess LLM decision (call `all_tool` tool or respond to the user)
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "all_tools",
        END: END,
    },
)

# Edges taken after the `action` node is called.
workflow.add_conditional_edges(
    "all_tools",
    # Assess agent decision
    grade_documents,
    # {
    #     "yes": "generate_answer",
    #     "no": "rewrite_question",
    # }
)
workflow.add_edge("generate_answer", END)
# workflow.add_edge("next_suggestions", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

checkpointer = InMemorySaver()

# Compile
graph = workflow.compile(checkpointer=checkpointer)

# graph = workflow.compile()


# -----------------------------
# RUNTIME CONFIG (ONE THREAD)
# -----------------------------
# config: RunnableConfig = {"configurable": {"thread_id": "1"}}

thread_id = str(uuid4())
config: RunnableConfig = {
    "configurable": {"thread_id": thread_id}
}
from langchain_core.messages import HumanMessage

if __name__ == "__main__":

    print("ðŸŸ¢ CCC Chatbot started")
    print("Type 'q' or 'quit' to exit\n")

    while True:
        user_input = input("You: ").strip()

        if user_input.lower() in {"q", "quit"}:
            print("ðŸ‘‹ Thanks Exiting chat .")
            break

        for chunk in graph.stream(
            {
                "messages": [
                    HumanMessage(content=user_input)
                ]
            },
            config=config
        ):
            for node, update in chunk.items():
                print(f"\nðŸ”¹ Update from node: {node}")

                for msg in update["messages"]:
                    if hasattr(msg, "pretty_print"):
                        msg.pretty_print()
                    else:
                        print(msg)

        print("\n" + "-" * 60 + "\n")
